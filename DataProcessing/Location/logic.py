import pandas as pd
import json
import re
from collections import Counter

# =========================================================
# ================= CONTEXT WORDS (INLINE) =================
# =========================================================
context_words = {
    # Prepositions / postpositions
    "–¥", "—Ç", "—Ä—É—É", "—Ä“Ø“Ø", "—Ö–∞–≤—å–¥", "–¥—ç—ç—Ä",
    "d", "t", "s", "ruu", "orchim", "hawiar", "habiar",
    "hawid", "havitsaa", "havit", "havid", "haviar",
    "havair", "der", "deer", "giin", "iin",

    # Movement verbs
    "—è–≤–Ω–∞", "—è–≤–∞—Ö",
    "yvna", "yvah", "ywh",

    # Conjunctions
    "yumu", "ymu", "—é–º", "—É—É",
    "eswel", "esvel", "—ç—Å–≤—ç–ª",

    # Address keywords
    "horoolol", "horooll", "—Ö–æ—Ä–æ–æ–ª–æ–ª",
    "xorooll", "xoroolol",
    "r", "—Ä",
    "dugeer", "dvgeer",
    "myangatad"
}

# =========================================================
# ======================= CONFIG ===========================
# =========================================================
POSTS_FILE = "number.xlsx"
DICTIONARY_FILE = "location_dictionary.json"

# =========================================================
# ==================== LOAD DATA ===========================
# =========================================================
print("”®–≥”©–≥–¥–ª–∏–π–≥ –∞—á–∞–∞–ª–∂ –±–∞–π–Ω–∞...")

posts_df = pd.read_excel(POSTS_FILE)

with open(DICTIONARY_FILE, encoding="utf-8") as f:
    location_dict = json.load(f)

# =========================================================
# =============== OPTIMIZED ALIAS INDEX ===================
# =========================================================
optimized_index = {}

def add_to_index(phrase, info):
    tokens = phrase.lower().strip().split()
    if not tokens:
        return

    first_word = tokens[0]
    optimized_index.setdefault(first_word, [])

    if not any(
        x["full_alias_tokens"] == tokens and x["canonical"] == info["canonical"]
        for x in optimized_index[first_word]
    ):
        optimized_index[first_word].append({
            "full_alias_tokens": tokens,
            "canonical": info["canonical"],
            "type": info.get("type", "standard"),
            "length": len(tokens)
        })

for loc in location_dict.values():
    for alias in loc.get("aliases", []):
        add_to_index(alias, loc)

# =========================================================
# ===================== NORMALIZE ==========================
# =========================================================
def normalize(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# =========================================================
# ================= HELPER FUNCTIONS ======================
# =========================================================
def check_suffix_context(token, alias_base):
    """
    13rhoroolol, 120t, 5ruu –≥—ç—Ö –º—ç—Ç suffix –∫–æ–Ω—Ç–µ–∫—Å—Ç —à–∞–ª–≥–∞—Ö
    """
    if token.startswith(alias_base) and len(token) > len(alias_base):
        suffix = token[len(alias_base):]
        return suffix in context_words
    return False


def check_common_logic(tokens, current_idx, match_len, alias_tokens):
    """
    COMMON —Ç”©—Ä”©–ª–¥ –∑–æ—Ä–∏—É–ª—Å–∞–Ω context-aware —à–∞–ª–≥–∞–ª—Ç
    """
    n = len(tokens)
    last_idx = current_idx + match_len - 1

    last_text_token = tokens[last_idx]
    last_alias_part = alias_tokens[-1]

    # CASE 1: suffix –¥–æ—Ç–æ—Ä context –±–∞–π–≥–∞–∞ —ç—Å—ç—Ö
    if check_suffix_context(last_text_token, last_alias_part):
        return True, 0

    # CASE 2: –¥–∞—Ä–∞–∞–≥–∏–π–Ω “Ø–≥ context —ç—Å—ç—Ö
    next_ptr = current_idx + match_len
    if next_ptr < n:
        next_token = tokens[next_ptr]

        if next_token in context_words:
            return True, 1

        # –¥–∞—Ä–∞–∞–≥–∏–π–Ω “Ø–≥ ”©”©—Ä –±–∞–π—Ä—à–∏–ª —ç—Ö–ª—ç–ª –±–∞–π–≤–∞–ª
        if next_token in optimized_index:
            return True, 0

    return False, 0

# =========================================================
# ================= CORE MATCH FUNCTION ===================
# =========================================================
def match_locations(text):
    text_norm = normalize(text)
    tokens = text_norm.split()
    if not tokens:
        return []

    found = set()
    n = len(tokens)
    i = 0

    while i < n:
        current_token = tokens[i]
        candidates = []

        for first_word in optimized_index:
            if current_token.startswith(first_word):
                candidates.extend(optimized_index[first_word])

        candidates.sort(key=lambda x: x["length"], reverse=True)

        matched = False
        for item in candidates:
            alias_tokens = item["full_alias_tokens"]
            m = len(alias_tokens)

            if i + m > n:
                continue

            # —ç—Ö–Ω–∏–π m-1 token —à–∞–ª–≥–∞—Ö
            if any(tokens[i + j] != alias_tokens[j] for j in range(m - 1)):
                continue

            last_text_token = tokens[i + m - 1]
            last_alias_part = alias_tokens[-1]

            if not last_text_token.startswith(last_alias_part):
                continue

            if item["type"] == "common":
                ok, skip = check_common_logic(tokens, i, m, alias_tokens)
                if ok:
                    found.add(item["canonical"])
                    i += m + skip
                    matched = True
                    break
            else:
                # STANDARD
                if len(last_text_token) <= len(last_alias_part) + 4:
                    found.add(item["canonical"])
                    i += m
                    matched = True
                    break

        if not matched:
            i += 1

    return list(found)

# =========================================================
# ================= EXECUTION & REPORT ====================
# =========================================================
print("–ë–∞–π—Ä—à–∏–ª —Ç–æ–≥—Ç–æ–æ–∂ –±–∞–π–Ω–∞...")

posts_df["matched_locations"] = posts_df["Content"].apply(match_locations)

posts_df.to_excel("posts_with_locations_logic.xlsx", index=False)

exploded = posts_df.explode("matched_locations").dropna(subset=["matched_locations"])

if not exploded.empty:
    counts = Counter(exploded["matched_locations"])
    total = len(exploded)

    stats = []
    for loc, cnt in counts.items():
        stats.append({
            "–ë–∞–π—Ä—à–∏–ª": loc,
            "–î–∞–≤—Ç–∞–º–∂": cnt,
            "–≠–∑–ª—ç—Ö —Ö—É–≤—å (%)": round(cnt / total * 100, 2)
        })

    summary_df = pd.DataFrame(stats).sort_values("–î–∞–≤—Ç–∞–º–∂", ascending=False)

    with pd.ExcelWriter("location_analysis_report.xlsx") as writer:
        posts_df.to_excel(writer, sheet_name="–ü–æ—Å—Ç—É—É–¥", index=False)
        summary_df.to_excel(writer, sheet_name="–ë–∞–π—Ä—à–ª—ã–Ω —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫", index=False)
        exploded[["ID", "matched_locations"]].to_excel(
            writer, sheet_name="–ò–ª—ç—Ä—Å—ç–Ω –±“Ø—Ö –±–∞–π—Ä—à–∏–ª", index=False
        )

    print("‚úÖ –ê–º–∂–∏–ª—Ç—Ç–∞–π –¥—É—É—Å–ª–∞–∞")
    print(f"üìç –ù–∏–π—Ç {total} –±–∞–π—Ä—à–∏–ª –∏–ª—ç—Ä—Å—ç–Ω")
else:
    print("‚ùå –ë–∞–π—Ä—à–∏–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
